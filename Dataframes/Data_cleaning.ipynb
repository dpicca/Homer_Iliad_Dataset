{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gisla\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#!pip install simpledorff\n",
    "import simpledorff as sf\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from glom import glom\n",
    "pd.__version__\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(json_path, annotator_int):\n",
    "    # load data using Python JSON module\n",
    "    with open(json_path,'r') as f:\n",
    "        data = json.loads(f.read())\n",
    "    # Flatten data, keep entry ID and person who completed the data\n",
    "    df_base_list = pd.json_normalize(data, record_path =['annotations', 'result'], meta = [\n",
    "        'id',\n",
    "        ['annotations', 'completed_by'],\n",
    "        ['annotations', 'id'],\n",
    "    ], record_prefix = '_',\n",
    "        errors = 'ignore'\n",
    "                                      )\n",
    "\n",
    "    # load original data separately\n",
    "    with open(json_path,'r') as f:\n",
    "        data = json.loads(f.read())\n",
    "    # Flatten data, keep entry ID and person who completed the data\n",
    "    df_original_data = pd.json_normalize(data, max_level=1, meta = ['id'], record_prefix = '_',\n",
    "        errors = 'ignore'\n",
    "                                      )\n",
    "\n",
    "    # Only keep relevant columns from df_original_data\n",
    "    series_id = df_original_data['id']\n",
    "    series_data = df_original_data['data.text']\n",
    "    df_original_data = pd.concat([series_id, series_data], axis=1)\n",
    "\n",
    "\n",
    "    # Add original data to dataframe\n",
    "    df_base_list = pd.merge(df_base_list, df_original_data, how='inner', left_on=['id'], right_on=['id'])\n",
    "\n",
    "    # Only keep relevant columns from df_base_list\n",
    "    series_datatype = df_base_list['_from_name']\n",
    "    series_start = df_base_list['_value.start']\n",
    "    series_end = df_base_list['_value.end']\n",
    "    series_text = df_base_list['_value.text']\n",
    "    series_label = df_base_list['_value.labels']\n",
    "    series_speakerid = df_base_list['_meta.text']\n",
    "    series_annotator = df_base_list['annotations.completed_by']\n",
    "    series_id = df_base_list['annotations.id']\n",
    "    series_original = df_base_list['data.text']\n",
    "    df_base_list = pd.concat([series_datatype, series_start, series_end, series_text, series_label, \n",
    "                              series_speakerid, series_annotator, series_id, series_original], axis=1)\n",
    "\n",
    "    # Rename remaining columns\n",
    "    df_base_list.columns = ['data_type', 'start', 'end', 'text', 'label', 'speaker_id', 'annotator', 'id', 'original_data']\n",
    "\n",
    "    # Anonymise annotator by giving them a number\n",
    "    df_base_list['annotator'] = annotator_int\n",
    "\n",
    "    # Remove useless lists in labels and convert to string\n",
    "    df_base_list['label'] = df_base_list['label'].str[0]\n",
    "    df_base_list['label'] = df_base_list['label'].astype(str)\n",
    "\n",
    "    # Separate speakers and speeches\n",
    "    df_speakers = df_base_list[df_base_list['label'] == \"Speaker\"]\n",
    "    df_speeches = df_base_list[df_base_list['label'] != \"Speaker\"]\n",
    "\n",
    "    #Clean speakers dataframe by removing unused columns\n",
    "    df_speakers = df_speakers.drop(['data_type', 'start', 'end', 'label', 'annotator', 'id', 'original_data'], axis=1)\n",
    "    # Remove useless list in speaker_id\n",
    "    df_speakers['speaker_id'] = df_speakers['speaker_id'].str[0]\n",
    "\n",
    "    # Replace NaN values with -1 values, so we can convert the columns to int\n",
    "    df_speeches['id'] = df_speeches['id'].fillna(-1)\n",
    "    df_speakers['speaker_id'] = df_speakers['speaker_id'].fillna(-1)\n",
    "\n",
    "    # Convert both columns to integer so we can merge later\n",
    "    df_speeches['id'] = df_speeches['id'].astype(int)\n",
    "    df_speakers['speaker_id'] = df_speakers['speaker_id'].astype(int)\n",
    "\n",
    "    # Merge speakers and speeches dataframes based on matching IDs\n",
    "    df_final = pd.merge(df_speeches, df_speakers, how='inner', left_on=['id'], right_on=['speaker_id'])\n",
    "\n",
    "    # Delete unused columns and rename new ones\n",
    "    df_final = df_final.drop(['speaker_id_x', 'speaker_id_y'], axis=1)\n",
    "    df_final.columns = ['data_type', 'start', 'end', 'text', 'label', 'annotator', 'id', 'original_data', 'speaker']\n",
    "\n",
    "    # Remove speech lines and only keep those with emotions\n",
    "    df_final = df_final[df_final['data_type'] == 'emotion']\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes(json_path_list):\n",
    "    dataframes = []\n",
    "    # Create the dataframes from json_path_list\n",
    "    for idx, file in enumerate(json_path_list):\n",
    "        df = create_dataframe(file, idx)\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Concatenate all dataframes into one\n",
    "    df = pd.concat(dataframes)\n",
    "    \n",
    "    # Create rounded length of text to use as a margin of error when grouping\n",
    "    df['rounded_length'] = (df['end'] - df['start']).round(-1)\n",
    "    \n",
    "    # Calculate Krippendorff's alpha\n",
    "    kripp = sf.calculate_krippendorffs_alpha_for_df(df,experiment_col='rounded_length', annotator_col='annotator', class_col='label')\n",
    "    print(kripp)\n",
    "    \n",
    "    # Group rows by original_data and length of text\n",
    "    df = df.groupby(['original_data', 'rounded_length'])\n",
    "    print(unique)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gisla\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\gisla\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13510140405616222\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'unique' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-5c5826bfcd74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mjson_list_24\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Jsonfiles/ch24_1.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Jsonfiles/ch24_2.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Jsonfiles/ch24_3.json'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mchapter_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_dataframes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_list_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mchapter_16\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_dataframes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_list_16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mchapter_17\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_dataframes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_list_17\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-fd6add8f2e42>\u001b[0m in \u001b[0;36mmerge_dataframes\u001b[1;34m(json_path_list)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Group rows by original_data and length of text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'original_data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rounded_length'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unique' is not defined"
     ]
    }
   ],
   "source": [
    "json_list_1 = ['Jsonfiles/ch1_1.json', 'Jsonfiles/ch1_2.json', 'Jsonfiles/ch1_3.json']\n",
    "json_list_16 = ['Jsonfiles/ch16_1.json', 'Jsonfiles/ch16_2.json', 'Jsonfiles/ch16_3.json']\n",
    "json_list_17 = ['Jsonfiles/ch17_1.json', 'Jsonfiles/ch17_3.json', 'Jsonfiles/ch17_4.json']\n",
    "json_list_18 = ['Jsonfiles/ch18_1.json', 'Jsonfiles/ch18_2.json']\n",
    "json_list_19 = ['Jsonfiles/ch19_1.json', 'Jsonfiles/ch19_2.json', 'Jsonfiles/ch19_3.json', 'Jsonfiles/ch19_4.json']\n",
    "json_list_20 = ['Jsonfiles/ch20_1.json', 'Jsonfiles/ch20_2.json', 'Jsonfiles/ch20_3.json']\n",
    "json_list_21 = ['Jsonfiles/ch21_1.json', 'Jsonfiles/ch21_2.json', 'Jsonfiles/ch21_3.json', 'Jsonfiles/ch21_3.json']\n",
    "json_list_22 = ['Jsonfiles/ch22_1.json', 'Jsonfiles/ch22_2.json', 'Jsonfiles/ch22_3.json']\n",
    "json_list_23 = ['Jsonfiles/ch23_1.json', 'Jsonfiles/ch23_2.json', 'Jsonfiles/ch23_3.json', 'Jsonfiles/ch23_4.json']\n",
    "json_list_24 = ['Jsonfiles/ch24_1.json', 'Jsonfiles/ch24_2.json', 'Jsonfiles/ch24_3.json']\n",
    "\n",
    "chapter_1 = merge_dataframes(json_list_1)\n",
    "chapter_16 = merge_dataframes(json_list_16)\n",
    "chapter_17 = merge_dataframes(json_list_17)\n",
    "chapter_18 = merge_dataframes(json_list_18)\n",
    "chapter_19 = merge_dataframes(json_list_19)\n",
    "chapter_20 = merge_dataframes(json_list_20)\n",
    "#chapter_21 = merge_dataframes(json_list_21)\n",
    "chapter_22 = merge_dataframes(json_list_22)\n",
    "chapter_23 = merge_dataframes(json_list_23)\n",
    "chapter_24 = merge_dataframes(json_list_24)\n",
    "\n",
    "# use this line to display dataframe   \n",
    "chapter_17.apply(lambda a: a[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

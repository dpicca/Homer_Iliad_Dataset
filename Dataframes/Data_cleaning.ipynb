{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simpledorff as sf\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "plt.rcParams[\"figure.figsize\"] = (40, 13)\n",
    "# Display Dataframes in full size\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "# Remove warnings\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(json_path, annotator_int):\n",
    "    # load data using Python JSON module\n",
    "    with open(json_path, 'r', encoding=\"utf-8\") as f:\n",
    "        data = json.loads(f.read())\n",
    "    # Flatten data, keep entry ID and person who completed the data\n",
    "    df_base_list = pd.json_normalize(\n",
    "        data, record_path=['annotations', 'result'], meta=[\n",
    "            'id',\n",
    "            ['annotations', 'completed_by'],\n",
    "            ['annotations', 'id'],\n",
    "        ], record_prefix='_',\n",
    "        errors='ignore'\n",
    "                                      )\n",
    "\n",
    "    # load original data separately\n",
    "    with open(json_path, 'r', encoding=\"utf-8\") as f:\n",
    "        data = json.loads(f.read())\n",
    "    # Flatten data, keep entry ID and person who completed the data\n",
    "    df_original_data = pd.json_normalize(\n",
    "        data, max_level=1, meta=['id'], record_prefix='_',\n",
    "        errors='ignore'\n",
    "                                      )\n",
    "\n",
    "    # Only keep relevant columns from df_original_data\n",
    "    series_id = df_original_data['id']\n",
    "    series_data = df_original_data['data.text']\n",
    "    df_original_data = pd.concat([series_id, series_data], axis=1)\n",
    "\n",
    "    # Add original data to dataframe\n",
    "    df_base_list = pd.merge(\n",
    "        df_base_list, df_original_data,\n",
    "        how='inner', left_on=['id'], right_on=['id'])\n",
    "\n",
    "    # Only keep relevant columns from df_base_list\n",
    "    series_datatype = df_base_list['_from_name']\n",
    "    series_start = df_base_list['_value.start']\n",
    "    series_end = df_base_list['_value.end']\n",
    "    series_text = df_base_list['_value.text']\n",
    "    series_label = df_base_list['_value.labels']\n",
    "    series_speakerid = df_base_list['_meta.text']\n",
    "    series_annotator = df_base_list['annotations.completed_by']\n",
    "    series_id = df_base_list['annotations.id']\n",
    "    series_original = df_base_list['data.text']\n",
    "    df_base_list = pd.concat(\n",
    "        [series_datatype, series_start, series_end, series_text, series_label,\n",
    "            series_speakerid, series_annotator,\n",
    "            series_id, series_original], axis=1)\n",
    "\n",
    "    # Rename remaining columns\n",
    "    df_base_list.columns = ['data_type', 'start', 'end', 'text', 'label',\n",
    "                            'speaker_id', 'annotator', 'id', 'original_data']\n",
    "\n",
    "    # Anonymise annotator by giving them a number\n",
    "    df_base_list['annotator'] = annotator_int\n",
    "\n",
    "    # Remove useless lists in labels and convert to string\n",
    "    df_base_list['label'] = df_base_list['label'].str[0]\n",
    "    df_base_list['label'] = df_base_list['label'].astype(str)\n",
    "\n",
    "    # Separate speakers and speeches\n",
    "    df_speakers = df_base_list[df_base_list['label'] == \"Speaker\"]\n",
    "    df_speeches = df_base_list[df_base_list['label'] != \"Speaker\"]\n",
    "\n",
    "    # Clean speakers dataframe by removing unused columns\n",
    "    df_speakers = df_speakers.drop(['data_type', 'start', 'end', 'label',\n",
    "                                    'annotator', 'id',\n",
    "                                    'original_data'], axis=1)\n",
    "\n",
    "    # Remove useless list in speaker_id\n",
    "    df_speakers['speaker_id'] = df_speakers['speaker_id'].str[0]\n",
    "\n",
    "    # Replace NaN values with -1 values, so we can convert the columns to int\n",
    "    df_speeches['id'] = df_speeches['id'].fillna(-1)\n",
    "    df_speakers['speaker_id'] = df_speakers['speaker_id'].fillna(-1)\n",
    "\n",
    "    # Convert both columns to integer so we can merge later\n",
    "    df_speeches['id'] = df_speeches['id'].astype(int)\n",
    "    df_speakers['speaker_id'] = df_speakers['speaker_id'].astype(int)\n",
    "\n",
    "    # Merge speakers and speeches dataframes based on matching IDs\n",
    "    df_final = pd.merge(df_speeches, df_speakers, how='inner',\n",
    "                        left_on=['id'], right_on=['speaker_id'])\n",
    "\n",
    "    # Delete unused columns and rename new ones\n",
    "    df_final = df_final.drop(['speaker_id_x', 'speaker_id_y'], axis=1)\n",
    "    df_final.columns = ['data_type', 'start', 'end', 'text', 'label',\n",
    "                        'annotator', 'id', 'original_data', 'speaker']\n",
    "\n",
    "    # Remove speech lines and only keep those with emotions\n",
    "    df_final = df_final[df_final['data_type'] == 'emotion']\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes(json_path_list):\n",
    "    dataframes = []\n",
    "    # Create the dataframes from json_path_list\n",
    "    for idx, file in enumerate(json_path_list):\n",
    "        df = create_dataframe(file, idx)\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Concatenate all dataframes into one\n",
    "    df = pd.concat(dataframes)\n",
    "\n",
    "    # Create rounded length of text to use as a margin of error when grouping\n",
    "    df['rounded_length'] = (df['end'] - df['start']).round(-1)\n",
    "\n",
    "    # Calculate Krippendorff's alpha\n",
    "    kripp = sf.calculate_krippendorffs_alpha_for_df(\n",
    "        df, experiment_col='rounded_length', annotator_col='annotator',\n",
    "        class_col='label')\n",
    "    print(kripp)\n",
    "\n",
    "    # Shorten original_data so that the graphs are readable\n",
    "    df_short = df\n",
    "    df_short['original_data'] = df_short['original_data'].str.slice(0, 50)\n",
    "    # Generate histograms grouped by paragraph, representing total of emotions\n",
    "    graph = df_short.hist(column='label', by='original_data',\n",
    "                          sharex=True, sharey=True)\n",
    "\n",
    "    # Group rows by original_data and length of text\n",
    "    df = df.groupby(['original_data', 'rounded_length'])\n",
    "\n",
    "    return df, kripp, graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def krippendorff_graph(list_values):\n",
    "    series_graph = pd.Series(list_values)\n",
    "    series_graph.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of JSONS grouped by chapters\n",
    "json_list_1 = ['Jsonfiles/ch1_1.json', 'Jsonfiles/ch1_2.json', 'Jsonfiles/ch1_3.json']\n",
    "json_list_16 = ['Jsonfiles/ch16_1.json', 'Jsonfiles/ch16_2.json', 'Jsonfiles/ch16_3.json']\n",
    "json_list_17 = ['Jsonfiles/ch17_1.json', 'Jsonfiles/ch17_3.json', 'Jsonfiles/ch17_4.json']\n",
    "json_list_18 = ['Jsonfiles/ch18_1.json', 'Jsonfiles/ch18_2.json']\n",
    "json_list_19 = ['Jsonfiles/ch19_1.json', 'Jsonfiles/ch19_2.json', 'Jsonfiles/ch19_3.json', 'Jsonfiles/ch19_4.json']\n",
    "json_list_20 = ['Jsonfiles/ch20_1.json', 'Jsonfiles/ch20_2.json', 'Jsonfiles/ch20_3.json']\n",
    "json_list_21 = ['Jsonfiles/ch21_1.json', 'Jsonfiles/ch21_2.json', 'Jsonfiles/ch21_3.json', 'Jsonfiles/ch21_3.json']\n",
    "json_list_22 = ['Jsonfiles/ch22_1.json', 'Jsonfiles/ch22_2.json', 'Jsonfiles/ch22_3.json']\n",
    "json_list_23 = ['Jsonfiles/ch23_1.json', 'Jsonfiles/ch23_2.json', 'Jsonfiles/ch23_3.json', 'Jsonfiles/ch23_4.json']\n",
    "json_list_24 = ['Jsonfiles/ch24_1.json', 'Jsonfiles/ch24_2.json', 'Jsonfiles/ch24_3.json']\n",
    "\n",
    "\n",
    "# Obtain dataframe, krippendorf value, and graph\n",
    "chapter_1, kripp_1, graph_1 = merge_dataframes(json_list_1)\n",
    "chapter_16, kripp_16, graph_16 = merge_dataframes(json_list_16)\n",
    "chapter_17, kripp_17, graph_17 = merge_dataframes(json_list_17)\n",
    "chapter_18, kripp_18, graph_18 = merge_dataframes(json_list_18)\n",
    "chapter_19, kripp_19, graph_19 = merge_dataframes(json_list_19)\n",
    "chapter_20, kripp_20, graph_20 = merge_dataframes(json_list_20)\n",
    "#chapter_21, kripp_21, graph_21 = merge_dataframes(json_list_21)\n",
    "chapter_22, kripp_22, graph_22 = merge_dataframes(json_list_22)\n",
    "chapter_23, kripp_23, graph_23 = merge_dataframes(json_list_23)\n",
    "chapter_24, kripp_24, graph_24 = merge_dataframes(json_list_24)\n",
    "\n",
    "# Get krippendorff graph\n",
    "krippendorff_series = [kripp_1, kripp_16, kripp_17, kripp_18, kripp_19, kripp_20, kripp_22, kripp_23, kripp_24]\n",
    "krippendorff_graph(krippendorff_series)\n",
    "\n",
    "# use this line to display dataframe\n",
    "chapter_17.apply(lambda a: a[:])\n",
    "\n",
    "chapters_dict = {\n",
    "    \"chapter_1\": chapter_1.apply(lambda a: a[:]),\n",
    "    \"chapter_16\": chapter_16.apply(lambda a: a[:]),\n",
    "    \"chapter_17\": chapter_17.apply(lambda a: a[:]),\n",
    "    \"chapter_18\": chapter_18.apply(lambda a: a[:]),\n",
    "    \"chapter_19\": chapter_19.apply(lambda a: a[:]),\n",
    "    \"chapter_20\": chapter_20.apply(lambda a: a[:]),\n",
    "    # \"chapter_21\": chapter_21.apply(lambda a: a[:]),\n",
    "    \"chapter_22\": chapter_22.apply(lambda a: a[:]),\n",
    "    \"chapter_23\": chapter_23.apply(lambda a: a[:]),\n",
    "    \"chapter_24\": chapter_24.apply(lambda a: a[:])\n",
    "    }\n",
    "\n",
    "# Export dataframes as pickle files\n",
    "for name, df in chapters_dict.items():\n",
    "    df.to_pickle(\"pickled_files/\" + name + \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_1 = chapter_1.apply(lambda a: a[:])\n",
    "chapter_16 = chapter_1.apply(lambda a: a[:])\n",
    "chapter_17 = chapter_1.apply(lambda a: a[:])\n",
    "chapter_18 = chapter_1.apply(lambda a: a[:])\n",
    "chapter_19 = chapter_1.apply(lambda a: a[:])\n",
    "chapter_20 = chapter_1.apply(lambda a: a[:])\n",
    "chapter_22 = chapter_1.apply(lambda a: a[:])\n",
    "chapter_23 = chapter_1.apply(lambda a: a[:])\n",
    "chapter_24 = chapter_1.apply(lambda a: a[:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

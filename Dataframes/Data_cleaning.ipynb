{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install simpledorff\n",
    "import simpledorff as sf\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from glom import glom\n",
    "pd.__version__\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(json_path, annotator_int):\n",
    "    # load data using Python JSON module\n",
    "    with open(json_path,'r') as f:\n",
    "        data = json.loads(f.read())\n",
    "    # Flatten data, keep entry ID and person who completed the data\n",
    "    df_base_list = pd.json_normalize(data, record_path =['annotations', 'result'], meta = [\n",
    "        'id',\n",
    "        ['annotations', 'completed_by'],\n",
    "        ['annotations', 'id'],\n",
    "    ], record_prefix = '_',\n",
    "        errors = 'ignore'\n",
    "                                      )\n",
    "\n",
    "    # load original data separately\n",
    "    with open(json_path,'r') as f:\n",
    "        data = json.loads(f.read())\n",
    "    # Flatten data, keep entry ID and person who completed the data\n",
    "    df_original_data = pd.json_normalize(data, max_level=1, meta = ['id'], record_prefix = '_',\n",
    "        errors = 'ignore'\n",
    "                                      )\n",
    "\n",
    "    # Only keep relevant columns from df_original_data\n",
    "    series_id = df_original_data['id']\n",
    "    series_data = df_original_data['data.text']\n",
    "    df_original_data = pd.concat([series_id, series_data], axis=1)\n",
    "\n",
    "\n",
    "    # Add original data to dataframe\n",
    "    df_base_list = pd.merge(df_base_list, df_original_data, how='inner', left_on=['id'], right_on=['id'])\n",
    "\n",
    "    # Only keep relevant columns from df_base_list\n",
    "    series_datatype = df_base_list['_from_name']\n",
    "    series_start = df_base_list['_value.start']\n",
    "    series_end = df_base_list['_value.end']\n",
    "    series_text = df_base_list['_value.text']\n",
    "    series_label = df_base_list['_value.labels']\n",
    "    series_speakerid = df_base_list['_meta.text']\n",
    "    series_annotator = df_base_list['annotations.completed_by']\n",
    "    series_id = df_base_list['annotations.id']\n",
    "    series_original = df_base_list['data.text']\n",
    "    df_base_list = pd.concat([series_datatype, series_start, series_end, series_text, series_label, \n",
    "                              series_speakerid, series_annotator, series_id, series_original], axis=1)\n",
    "\n",
    "    # Rename remaining columns\n",
    "    df_base_list.columns = ['data_type', 'start', 'end', 'text', 'label', 'speaker_id', 'annotator', 'id', 'original_data']\n",
    "\n",
    "    # Anonymise annotator by giving them a number\n",
    "    df_base_list['annotator'] = annotator_int\n",
    "\n",
    "    # Remove useless lists in labels and convert to string\n",
    "    df_base_list['label'] = df_base_list['label'].str[0]\n",
    "    df_base_list['label'] = df_base_list['label'].astype(str)\n",
    "\n",
    "    # Separate speakers and speeches\n",
    "    df_speakers = df_base_list[df_base_list['label'] == \"Speaker\"]\n",
    "    df_speeches = df_base_list[df_base_list['label'] != \"Speaker\"]\n",
    "\n",
    "    #Clean speakers dataframe by removing unused columns\n",
    "    df_speakers = df_speakers.drop(['data_type', 'start', 'end', 'label', 'annotator', 'id', 'original_data'], axis=1)\n",
    "    # Remove useless list in speaker_id\n",
    "    df_speakers['speaker_id'] = df_speakers['speaker_id'].str[0]\n",
    "\n",
    "    # Replace NaN values with -1 values, so we can convert the columns to int\n",
    "    df_speeches['id'] = df_speeches['id'].fillna(-1)\n",
    "    df_speakers['speaker_id'] = df_speakers['speaker_id'].fillna(-1)\n",
    "\n",
    "    # Convert both columns to integer so we can merge later\n",
    "    df_speeches['id'] = df_speeches['id'].astype(int)\n",
    "    df_speakers['speaker_id'] = df_speakers['speaker_id'].astype(int)\n",
    "\n",
    "    # Merge speakers and speeches dataframes based on matching IDs\n",
    "    df_final = pd.merge(df_speeches, df_speakers, how='inner', left_on=['id'], right_on=['speaker_id'])\n",
    "\n",
    "    # Delete unused columns and rename new ones\n",
    "    df_final = df_final.drop(['speaker_id_x', 'speaker_id_y'], axis=1)\n",
    "    df_final.columns = ['data_type', 'start', 'end', 'text', 'label', 'annotator', 'id', 'original_data', 'speaker']\n",
    "\n",
    "    # Remove speech lines and only keep those with emotions\n",
    "    df_final = df_final[df_final['data_type'] == 'emotion']\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes(json_path_list):\n",
    "    dataframes = []\n",
    "    # Create the dataframes from json_path_list\n",
    "    for idx, file in enumerate(json_path_list):\n",
    "        df = create_dataframe(file, idx)\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Concatenate all dataframes into one\n",
    "    df = pd.concat(dataframes)\n",
    "    \n",
    "    # Create rounded length of text to use as a margin of error when grouping\n",
    "    df['rounded_length'] = (df['end'] - df['start']).round(-1)\n",
    "    \n",
    "    # Calculate Krippendorff's alpha\n",
    "    kripp = sf.calculate_krippendorffs_alpha_for_df(df,experiment_col='rounded_length', annotator_col='annotator', class_col='label')\n",
    "    print(kripp)\n",
    "    \n",
    "    # Group rows by original_data and length of text\n",
    "    df = df.groupby(['original_data', 'rounded_length'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13510140405616222\n",
      "0.14919852034525272\n",
      "0.09013785790031814\n",
      "0.4722222222222222\n",
      "0.32815890502420286\n",
      "0.291497975708502\n",
      "0.1682926829268293\n",
      "0.20194884287454318\n",
      "0.16258351893095768\n",
      "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f5ad4102080>\n"
     ]
    }
   ],
   "source": [
    "json_list_1 = ['Jsonfiles/ch1_1.json', 'Jsonfiles/ch1_2.json', 'Jsonfiles/ch1_3.json']\n",
    "json_list_16 = ['Jsonfiles/ch16_1.json', 'Jsonfiles/ch16_2.json', 'Jsonfiles/ch16_3.json']\n",
    "json_list_17 = ['Jsonfiles/ch17_1.json', 'Jsonfiles/ch17_3.json', 'Jsonfiles/ch17_4.json']\n",
    "json_list_18 = ['Jsonfiles/ch18_1.json', 'Jsonfiles/ch18_2.json']\n",
    "json_list_19 = ['Jsonfiles/ch19_1.json', 'Jsonfiles/ch19_2.json', 'Jsonfiles/ch19_3.json', 'Jsonfiles/ch19_4.json']\n",
    "json_list_20 = ['Jsonfiles/ch20_1.json', 'Jsonfiles/ch20_2.json', 'Jsonfiles/ch20_3.json']\n",
    "json_list_21 = ['Jsonfiles/ch21_1.json', 'Jsonfiles/ch21_2.json', 'Jsonfiles/ch21_3.json', 'Jsonfiles/ch21_3.json']\n",
    "json_list_22 = ['Jsonfiles/ch22_1.json', 'Jsonfiles/ch22_2.json', 'Jsonfiles/ch22_3.json']\n",
    "json_list_23 = ['Jsonfiles/ch23_1.json', 'Jsonfiles/ch23_2.json', 'Jsonfiles/ch23_3.json', 'Jsonfiles/ch23_4.json']\n",
    "json_list_24 = ['Jsonfiles/ch24_1.json', 'Jsonfiles/ch24_2.json', 'Jsonfiles/ch24_3.json']\n",
    "\n",
    "chapter_1 = merge_dataframes(json_list_1)\n",
    "chapter_16 = merge_dataframes(json_list_16)\n",
    "chapter_17 = merge_dataframes(json_list_17)\n",
    "chapter_18 = merge_dataframes(json_list_18)\n",
    "chapter_19 = merge_dataframes(json_list_19)\n",
    "chapter_20 = merge_dataframes(json_list_20)\n",
    "# chapter_21 = merge_dataframes(json_list_21)\n",
    "chapter_22 = merge_dataframes(json_list_22)\n",
    "chapter_23 = merge_dataframes(json_list_23)\n",
    "chapter_24 = merge_dataframes(json_list_24)\n",
    "\n",
    "# use this line to display dataframe   \n",
    "# applied_chapter_17 = chapter_17.apply(lambda a: a[:])\n",
    "\n",
    "chapters_dict = {\n",
    "    \"chapter_1\": chapter_1.apply(lambda a: a[:]),\n",
    "    \"chapter_16\": chapter_16.apply(lambda a: a[:]),\n",
    "    \"chapter_17\": chapter_17.apply(lambda a: a[:]),\n",
    "    \"chapter_18\": chapter_18.apply(lambda a: a[:]),\n",
    "    \"chapter_19\": chapter_19.apply(lambda a: a[:]),\n",
    "    \"chapter_20\": chapter_20.apply(lambda a: a[:]),\n",
    "    # \"chapter_21\": chapter_21.apply(lambda a: a[:]),\n",
    "    \"chapter_22\": chapter_22.apply(lambda a: a[:]),\n",
    "    \"chapter_23\": chapter_23.apply(lambda a: a[:]),\n",
    "    \"chapter_24\": chapter_24.apply(lambda a: a[:])\n",
    "}\n",
    "\n",
    "# Export dataframes as pickle files\n",
    "for name, df in chapters_dict.items():\n",
    "    df.to_pickle(\"pickled_files/\" + name + \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
